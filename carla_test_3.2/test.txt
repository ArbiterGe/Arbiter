这个版本讲目标进行调整，采用固定的目标位置，固定的初始位置，看是否有用
2021.1.21调整了模型，增加了损失函数中policy的比重。在训练结果上来看并不是很理想。现在通过更改速度的比重，将速度变化，路程变化变为绝对值，看是否有所变化。
如果仍有问题需要好好的探讨一下奖励，以及奖励比重。以及室友是因为训练序列较短导致的
2021.1.21-10：58 参数与之前一致

2021.1.22 
在更改batchsize与replaybuffer之后发现有一定的效果，但仍然无法达到收敛状态，现在重新的思考一下奖励。
1.25
调整过训练结束机制，将训练过程中速度较小项进行调整。
更改了输入图像的大小由64*64转换为640*640，在这种情况下增加了图像的更多细节，但也对训练增加了难度，后发现batch——size过大，严重占内存，要调整batch——size大小，尽管调整了batchsize的大小后发现了其他问题，batchsize调整为内存可以应用的阶段，但却发现每个训练帧之间的时间过长，service的反应时间过长，以至于当车放生碰撞时会有较长的时间去响应。
1.26
调整batch——size的大小，从64调整了32，看训练效果。
1.28
tianjia pygame
xiaoguo bujia yiquxiao 
2.5 目标分解调整奖励尺寸
版本，2月28日将调整为多线程模型，将训练与agent更新分开，但发现内存溢出情况，
怀疑是replaybuffer问题，replaybuffer过大
3.3-3.5：调整replaybuffer为存储模型，将replaybuffer保存为hdf5格式文件，
初期思路将RB中的列表，存储至hdf5文件中，学习hdf5文件的存储方式及思想，发现其存储更多为numpy格式，
列表格式中的元素存在字典，因此是hdf5格式中无法存储的，调整存储思想，hdf5格式更类似与字典形式，为key与字节，从list思想转化为字典思想，将list中的序列作为字典中的key，以分组的方式进行存储，可以实现存储。
但并不了解存储时的时间，以及文件大小，读取时间及大小需要进一步的测试与验证。
